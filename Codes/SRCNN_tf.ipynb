{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SRCNN_tf","version":"0.3.2","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2kqMgN3hnl2v","colab_type":"text"},"source":["# **Implementation of a Super-Resolution Convolutional Neural Network (SRNN)**\n","\n","### References\n","*   Image Super-Resolution using Deep Convolutional Neural Networks (paper)\n","*   ogreen8084/srcnn_part1 (github)\n","\n","### Steps\n","*   Import datasets\n","*   Preproce\n","*   Create architecture\n","*   Train model"]},{"cell_type":"markdown","metadata":{"id":"3xV8nGMfSom1","colab_type":"text"},"source":["## Import Datasets"]},{"cell_type":"code","metadata":{"id":"Bh35J-T-MnNH","colab_type":"code","colab":{}},"source":["!git clone https://github.com/luizmanke/image-super-resolution.git\n","!mkdir -p data/input\n","!cp image-super-resolution/Datasets/Yang\\ et\\ al/*.bmp data/input"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HQPVL1AzoV2z"},"source":["## Preprocess Images"]},{"cell_type":"code","metadata":{"id":"aeLoVftYp0GT","colab_type":"code","colab":{}},"source":["# Import system libraries\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import tensorflow as tf\n","from PIL import Image\n","from sklearn.utils import shuffle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Hjj446zoaBm","colab_type":"code","colab":{}},"source":["def reformat_image_and_blur(image_file_path, file_name, folder_name, blur=False, size=(128,128), dsfactor=2):\n","    size = size\n","    new_size = size[0] // dsfactor, size[1] //dsfactor\n","    if \"._\" in file_name:\n","        file_name = file_name[2:]\n","    image = Image.open(os.path.join(image_file_path,file_name))\n","    image_size = image.size\n","    width = image_size[0]\n","    height = image_size[1]\n","    bigside = width if width > height else height\n","    background = Image.new('RGB', (bigside, bigside), (255, 255, 255))\n","    offset = (int(round(((bigside - width) / 2), 0)), int(round(((bigside - height) / 2), 0)))\n","    background.paste(image, offset)\n","    if blur:\n","        background.thumbnail(new_size, Image.ANTIALIAS)\n","    back = background.resize(size, Image.BICUBIC)\n","    back = np.array(back)\n","    return back/255."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"okwlcp_9ppgS","colab_type":"code","colab":{}},"source":["hr_imgs = []\n","bi_imgs = []\n","file_path = \"data/input\"\n","\n","for shoe_pic in os.listdir(file_path):\n","  hr = reformat_image_and_blur(file_path, shoe_pic, '')\n","  hr_imgs.append(hr)\n","  bicubic = reformat_image_and_blur(file_path, shoe_pic, '', blur=True)\n","  bi_imgs.append(bicubic)\n","hr_imgs = np.array(hr_imgs)\n","bi_imgs = np.array(bi_imgs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y57P3zsmrCcp","colab_type":"code","colab":{}},"source":["train_pct = 0.8\n","n_train = int(train_pct * len(hr_imgs))\n","\n","x_train = bi_imgs[:n_train]\n","y_train = hr_imgs[:n_train]\n","x_test = bi_imgs[n_train:]\n","y_test = hr_imgs[n_train:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"olTBRJqoExQC","colab_type":"text"},"source":["## Create Architecture"]},{"cell_type":"code","metadata":{"id":"PhFE-vy3rYk9","colab_type":"code","colab":{}},"source":["batch_size = 19\n","N = bi_imgs.shape[0]\n","num_batches = N // batch_size"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wVF46VkKrj0l","colab_type":"code","colab":{}},"source":["def conv_layer(input_, filters, bias, alpha=0.2, final_layer=False, strides=[1,1,1,1]):\n","    conv1 = tf.nn.conv2d(input_, filters, strides, padding='SAME')\n","    conv1 = tf.nn.bias_add(conv1, bias)\n","    if not final_layer:\n","        return tf.nn.leaky_relu(conv1, alpha )\n","    else:\n","        return tf.tanh(conv1)\n","\n","def deconv_layer(inputs, filters, bias, output_shape, alpha=0.1, final_layer=False, strides=[1,1,1,1], padding='SAME'):\n","    deconv1 = tf.nn.conv2d_transpose(inputs, filters, output_shape, strides, padding)\n","    deconv1 = tf.nn.bias_add(deconv1, bias)\n","    if not final_layer:\n","        \n","        return tf.nn.leaky_relu(deconv1, alpha)\n","    else:\n","        return tf.tanh(deconv1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s18NEhEv0IPX","colab_type":"code","colab":{}},"source":["def init_filter(shape):\n","    w = np.random.randn(*shape)/ np.sqrt(np.prod(shape[:-1]) + np.prod(shape[:-2])*shape[-1])\n","    return tf.Variable(w, dtype=np.float32)\n","\n","def init_bias(shape):\n","    b = np.zeros(shape[-1])\n","    return tf.Variable(b, dtype=np.float32)\n","\n","def init_bias_deconv(shape):\n","    b = np.zeros(shape[2])\n","    return tf.Variable(b, dtype=np.float32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QwOcSwzh0L9A","colab_type":"code","colab":{}},"source":["def next_batch(inputs, labels, batch_size, num_batches):\n","    for num in range(num_batches):\n","        inputs_batch = inputs[num*batch_size:(num+1)*batch_size]\n","        labels_batch = labels[num*batch_size:(num+1)*batch_size]\n","        yield np.array(inputs_batch), np.array(labels_batch)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qfjlONz30QJc","colab_type":"code","colab":{}},"source":["inputs = tf.placeholder(tf.float32, [None, 128, 128, 3])\n","labels = tf.placeholder(tf.float32, [None, 128, 128, 3])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"67qm0iIu0ZS3","colab_type":"code","colab":{}},"source":["w1_init = [5,5,32,3]\n","w1 = init_filter(w1_init)\n","b1 = init_bias_deconv(w1_init)\n","\n","w2_init = [5,5,32,32]\n","w2 = init_filter(w2_init)\n","b2 = init_bias_deconv(w2_init)\n","\n","w3_init = [5,5,32,32]\n","w3 = init_filter(w3_init)\n","b3 = init_bias_deconv(w3_init)\n","\n","w4_init = [3,3,32,32]\n","w4 = init_filter(w4_init)\n","b4 = init_bias_deconv(w4_init)\n","\n","w5_init = [3,3,32,32]\n","w5 = init_filter(w5_init)\n","b5 = init_bias_deconv(w5_init)\n","\n","w6_init = [3,3,32,32]\n","w6 = init_filter(w6_init)\n","b6 = init_bias_deconv(w6_init)\n","\n","w7_init = [3,3,3,32]\n","w7 = init_filter(w7_init)\n","b7= init_bias_deconv(w7_init)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2nct9_qx0fS-","colab_type":"code","colab":{}},"source":["conv_layer1 = deconv_layer(inputs, w1, b1, [batch_size, 128, 128, 32])\n","conv_layer2 = deconv_layer(conv_layer1, w2, b2,[batch_size, 128, 128, 32])\n","conv_layer3 = deconv_layer(conv_layer2, w3, b3,[batch_size, 128, 128, 32])\n","conv_layer4 = deconv_layer(conv_layer3, w4, b4,[batch_size, 128, 128, 32])\n","conv_layer5 = deconv_layer(conv_layer4, w5, b5,[batch_size, 128, 128, 32])\n","conv_layer6 = deconv_layer(conv_layer5, w6, b6,[batch_size, 128, 128, 32])\n","pred = deconv_layer(conv_layer6, w7, b7, [batch_size, 128, 128, 3], final_layer=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PKSvJxQdE1ip","colab_type":"text"},"source":["## Train Model"]},{"cell_type":"code","metadata":{"id":"tZuV1iZ00N44","colab_type":"code","colab":{}},"source":["def show_test_images(blurrys, samples, actuals):\n","    print('BiCubic    SRCNN (Perpetual Loss)     Ground Truth')\n","    rows, cols = 9, 3\n","    fig, axes = plt.subplots(figsize=(10, 10), nrows=rows, ncols=cols, sharex=True, sharey=True)\n","    samples2 = []\n","    blurrys2 = []\n","    actuals2 = []\n","    \n","    for blurry in blurrys:\n","        blurrys2.append((blurry*255).astype(np.uint8))\n","\n","    for sample in samples[0]:\n","        samples2.append(((sample /2.0 + 0.5)*255).astype(np.uint8))\n","    for actual in actuals:\n","        actuals2.append((actual*255).astype(np.uint8))\n","    \n","    #to iterate through the images\n","    a = 0\n","    b = 1\n","    for ax_row in axes:\n","        for ax in ax_row:\n","            if b % 3 == 1:\n","                ax.imshow(blurrys2[a])\n","            elif b % 3 == 2:\n","                ax.imshow(samples2[a])\n","            elif b % 3 == 0:\n","                ax.imshow(actuals2[a])\n","            ax.xaxis.set_visible(False)\n","            ax.yaxis.set_visible(False)\n","            if b % 3 == 0:\n","                a+=1\n","            b += 1\n","\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3iyag250kP6","colab_type":"code","colab":{}},"source":["loss = tf.reduce_mean(tf.square(labels - pred))\n","train_op = tf.train.AdamOptimizer(0.00005).minimize(loss)\n","saver = tf.train.Saver()\n","epochs = 1001\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    for e in range(epochs):\n","        for i in range(num_batches):\n","            batch = next_batch(x_train, y_train, batch_size, num_batches)\n","            inputs_, labels_ = next(batch)\n","            labels_ = (labels_ - 0.5) * 2\n","            \n","            train_loss,_ = sess.run([loss, train_op], feed_dict={inputs:inputs_, labels:labels_})\n","        if (e % 20 == 0) & (e % 200 != 0):\n","            print(\"Epoch: %d train loss: %f\" %(e, float(train_loss)))\n","        if e % 200 == 0:\n","            print(\"Epoch: %d train loss: %f\" %(e, float(train_loss)))\n","            gen_samples = sess.run([pred],feed_dict={inputs:x_test})\n","            show_test_images(x_test, gen_samples, y_test)\n","        np.random.shuffle([inputs_, labels_])"],"execution_count":0,"outputs":[]}]}